{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JvuFetZOlqUW",
        "5q5luEyJjhdy",
        "le8go0Ho0TH3",
        "BDVHPHJA5ZQJ",
        "XbnnnwNf0owO",
        "5h92P5nk0s4U",
        "csXOe2wV0ue3",
        "Esm-bmotzoB-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZ1te0DIgd2"
      },
      "source": [
        "# from google.colab import drive\n",
        "#\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVHZ8_8hluLO"
      },
      "source": [
        "### Preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR81ABgmmlu5"
      },
      "source": [
        "# Change working directory to the root of the project\n",
        "# %cd ...\n",
        "# !pwd\n",
        "\n",
        "# Unzip data\n",
        "# !unzip 2021VRDL_HW1_datasets.zip\n",
        "# !unzip testing_images.zip -d testing\n",
        "# !unzip training_images.zip -d training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdmTTx8pKhPr"
      },
      "source": [
        "from torchvision.utils import make_grid, save_image\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDMUOXZvUOl3"
      },
      "source": [
        "LR = 0.001\n",
        "\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/2021VRDL/HW1/training\"\n",
        "TEST_IMG_DIR = \"/content/drive/MyDrive/2021VRDL/HW1/testing\"\n",
        "TRAIN_LABEL_FILE = \"/content/drive/MyDrive/2021VRDL/HW1/training_labels.txt\"\n",
        "TEST_FILE = \"/content/drive/MyDrive/2021VRDL/HW1/testing_img_order.txt\"\n",
        "CLASSES_FILE = \"/content/drive/MyDrive/2021VRDL/HW1/classes.txt\"\n",
        "MODEL_SAVE_DIR = \"/content/drive/MyDrive/2021VRDL/HW1/models\"\n",
        "PREDICTION_DIR = \"/content/drive/MyDrive/2021VRDL/HW1/prediction\"\n",
        "TRAIN_NAME_FILE = \"/content/drive/MyDrive/2021VRDL/HW1/train_filenames.txt\"\n",
        "VALID_NAME_FILE = \"/content/drive/MyDrive/2021VRDL/HW1/valid_filenames.txt\"\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "N_CLASSES = 200\n",
        "print(\"Current device:\", DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS913NRpO5vn"
      },
      "source": [
        "# Load class and make label\n",
        "CLS_LABEL_TO_NAME = {\n",
        "    i: c\n",
        "    for i, c in enumerate(pd.read_csv(\n",
        "        CLASSES_FILE, names=[\"class\"])[\"class\"].to_list())\n",
        "}\n",
        "CLS_NAME_TO_LABEL = {CLS_LABEL_TO_NAME[i]: i for i in CLS_LABEL_TO_NAME}\n",
        "\n",
        "\n",
        "# Load training data's labels\n",
        "def read_label_as_dict(name_to_label_file):\n",
        "    df = pd.read_csv(TRAIN_LABEL_FILE, sep=\" \",\n",
        "                     names=[\"filename\", \"class\"])\n",
        "    df.index = df[\"filename\"].tolist()\n",
        "    return df[\"class\"].to_dict()\n",
        "\n",
        "\n",
        "TRAIN_NAME_TO_CLS_NAME = read_label_as_dict(TRAIN_LABEL_FILE)\n",
        "TRAIN_NAME_TO_CLS_LABEL = {\n",
        "    key: CLS_NAME_TO_LABEL[TRAIN_NAME_TO_CLS_NAME[key]]\n",
        "    for key in TRAIN_NAME_TO_CLS_NAME\n",
        "}\n",
        "TRAIN_NAMES = [key for key in TRAIN_NAME_TO_CLS_LABEL]\n",
        "TEST_NAMES = pd.read_csv(TEST_FILE, names=[\"filename\"])[\"filename\"].to_list()\n",
        "TRAIN_X = (\n",
        "    pd.read_csv(TRAIN_NAME_FILE,\n",
        "                names=[\"filename\"]).to_numpy().reshape(-1).tolist()\n",
        ")  # 2600, each class: 13\n",
        "VALID_X = (\n",
        "    pd.read_csv(VALID_NAME_FILE,\n",
        "                names=[\"filename\"]).to_numpy().reshape(-1).tolist()\n",
        ")  # 400, each class: 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPNPDshbd7Ni",
        "outputId": "dd1c7db4-a1cb-4175-8727-154028105868"
      },
      "source": [
        "# Use more data to train\n",
        "tmp_t_x = TRAIN_X\n",
        "tmp_v_x = []\n",
        "for i, tmp_item in enumerate(VALID_X):\n",
        "    if i % 2 == 0:\n",
        "        tmp_t_x.append(tmp_item)\n",
        "    else:\n",
        "        tmp_v_x.append(tmp_item)\n",
        "print(len(tmp_t_x), len(tmp_v_x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2800 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDeS16KlNVoS"
      },
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, img_dir, img_names, img_name_to_label,\n",
        "                 transform, test=False):\n",
        "        super().__init__()\n",
        "        self.img_dir, self.img_names, self.img_name_to_label = (\n",
        "            img_dir,\n",
        "            img_names,\n",
        "            img_name_to_label,\n",
        "        )\n",
        "        self.transform = transform\n",
        "        self.test = test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_arr = Image.open(\n",
        "            os.path.join(self.img_dir, self.img_names[idx])).convert(\"RGB\")\n",
        "        if self.test is True:\n",
        "            img_tensor = self.transform(img_arr)\n",
        "            return img_tensor\n",
        "        else:\n",
        "            img_label = self.img_name_to_label[self.img_names[idx]]\n",
        "            img_tensor = self.transform(img_arr)\n",
        "            return img_tensor, img_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvuFetZOlqUW"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhC2g3ydt_GN"
      },
      "source": [
        "BATCH_SIZE = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhJcghllZRBB"
      },
      "source": [
        "# Mean: (0.4819434270441397, 0.49756442238151166, 0.4321436327680999)\n",
        "# Std: (0.1327144718474103, 0.12794266425102524, 0.1728887645128543)\n",
        "\n",
        "###############\n",
        "\n",
        "\n",
        "# TRAIN_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#     # transforms.RandomHorizontalFlip(p=0.5),\n",
        "#     # transforms.RandomRotation(45),\n",
        "#     # transforms.Resize((100, 100)),\n",
        "#     transforms.RandomCrop((45, 45)),\n",
        "# ])\n",
        "# VALID_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((45, 45))])\n",
        "# TEST_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((45, 45))])\n",
        "\n",
        "# TRAIN_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#     transforms.RandomRotation(30),\n",
        "#     transforms.RandomHorizontalFlip(p=0.5),\n",
        "#     transforms.Resize((256, 256)),\n",
        "#     transforms.RandomCrop((224, 224)),\n",
        "# ])\n",
        "# VALID_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((224, 224))])\n",
        "# TEST_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((224, 224))])\n",
        "\n",
        "\n",
        "# TRAIN_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                     #   transforms.RandomRotation(30),\n",
        "#                                       transforms.Resize((400, 400)),\n",
        "#                                       transforms.RandomRotation(45),\n",
        "#                                       transforms.RandomCrop((360, 360)),\n",
        "#                                       # transforms.Resize((224, 224))\n",
        "#                                      ])\n",
        "# VALID_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((360, 360))])\n",
        "# TEST_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((360, 360))])\n",
        "\n",
        "TRAIN_TRANSFORM = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        #   transforms.RandomRotation(30),\n",
        "        transforms.Resize((324, 324)),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomCrop((300, 300)),\n",
        "        transforms.Normalize((0.48194, 0.49756, 0.43214),\n",
        "                             (0.13271, 0.12794, 0.17289))\n",
        "        # transforms.Resize((224, 224))\n",
        "    ]\n",
        ")\n",
        "VALID_TRANSFORM = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((324, 324)),\n",
        "        transforms.CenterCrop((300, 300)),\n",
        "        transforms.Normalize((0.48194, 0.49756, 0.43214),\n",
        "                             (0.13271, 0.12794, 0.17289)),\n",
        "    ]\n",
        ")\n",
        "TEST_TRANSFORM = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        #   transforms.Resize((300, 300)),\n",
        "        transforms.Resize((324, 324)),\n",
        "        #   transforms.RandomRotation(20),\n",
        "        #   transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.CenterCrop((300, 300)),\n",
        "        transforms.Normalize((0.48194, 0.49756, 0.43214),\n",
        "                             (0.13271, 0.12794, 0.17289)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# TRAIN_TRANSFORM = transforms.Compose([\n",
        "#  transforms.ToTensor(),\n",
        "#  transforms.Resize((256, 256)),\n",
        "#  transforms.RandomRotation(30),\n",
        "#  transforms.RandomHorizontalFlip(p=0.5),\n",
        "#  transforms.RandomCrop((224, 224)),\n",
        "# ])\n",
        "# VALID_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((224, 224))])\n",
        "# TEST_TRANSFORM = transforms.Compose([transforms.ToTensor(),\n",
        "#                                       transforms.Resize((224, 224))])\n",
        "\n",
        "\n",
        "# TRAIN_X, VALID_X = train_test_split(TRAIN_NAMES, test_size=0.2)\n",
        "TRAIN_DATASET = BirdDataset(\n",
        "    img_dir=TRAIN_IMG_DIR,\n",
        "    img_names=TRAIN_X,\n",
        "    img_name_to_label=TRAIN_NAME_TO_CLS_LABEL,\n",
        "    transform=TRAIN_TRANSFORM,\n",
        ")\n",
        "VALID_DATASET = BirdDataset(\n",
        "    img_dir=TRAIN_IMG_DIR,\n",
        "    img_names=VALID_X,\n",
        "    img_name_to_label=TRAIN_NAME_TO_CLS_LABEL,\n",
        "    transform=VALID_TRANSFORM,\n",
        ")\n",
        "TEST_DATASET = BirdDataset(\n",
        "    img_dir=TEST_IMG_DIR,\n",
        "    img_names=TEST_NAMES,\n",
        "    img_name_to_label=TRAIN_NAME_TO_CLS_LABEL,\n",
        "    transform=TEST_TRANSFORM,\n",
        "    test=True,\n",
        ")\n",
        "TRAIN_DATALOADER = DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "VALID_DATALOADER = DataLoader(VALID_DATASET, batch_size=BATCH_SIZE,\n",
        "                              shuffle=False)\n",
        "TEST_DATALOADER = DataLoader(TEST_DATASET, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "g8gIUb31BuZE"
      },
      "source": [
        "# # Install EfficientNet\n",
        "# !pip install efficientnet_pytorch\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "# MODEL = EfficientNet.from_pretrained('efficientnet-b5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q5luEyJjhdy"
      },
      "source": [
        "### Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OuzqRA3xrF5"
      },
      "source": [
        "#\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "# Bilinear Attention Pooling\n",
        "class BAP(nn.Module):\n",
        "    def __init__(self, pool=\"GAP\"):\n",
        "        super(BAP, self).__init__()\n",
        "        assert pool in [\"GAP\"]\n",
        "        self.pool = None\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "    def forward(self, features, attentions):\n",
        "        B, C, H, W = features.size()\n",
        "        _, M, AH, AW = attentions.size()\n",
        "\n",
        "        # match size\n",
        "        if AH != H or AW != W:\n",
        "            attentions = F.upsample_bilinear(attentions, size=(H, W))\n",
        "\n",
        "        # feature_matrix: (B, M, C) -> (B, M * C)\n",
        "        feature_matrix = (\n",
        "            torch.einsum(\"imjk,injk->imn\",\n",
        "                         (attentions, features)) / float(H * W)\n",
        "        ).view(B, -1)\n",
        "\n",
        "        # sign-sqrt\n",
        "        feature_matrix_raw = torch.sign(feature_matrix) * torch.sqrt(\n",
        "            torch.abs(feature_matrix) + self.epsilon\n",
        "        )\n",
        "\n",
        "        # l2 normalization along dimension M and C\n",
        "        feature_matrix = F.normalize(feature_matrix_raw, dim=-1)\n",
        "\n",
        "        if self.training:\n",
        "            fake_att = torch.zeros_like(attentions).uniform_(0, 2)\n",
        "        else:\n",
        "            fake_att = torch.ones_like(attentions)\n",
        "        counterfactual_feature = (\n",
        "            torch.einsum(\"imjk,injk->imn\",\n",
        "                         (fake_att, features)) / float(H * W)\n",
        "        ).view(B, -1)\n",
        "\n",
        "        counterfactual_feature = torch.sign(\n",
        "            counterfactual_feature) * torch.sqrt(\n",
        "            torch.abs(counterfactual_feature) + self.epsilon\n",
        "        )\n",
        "\n",
        "        counterfactual_feature = F.normalize(counterfactual_feature, dim=-1)\n",
        "        return feature_matrix, counterfactual_feature\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)\n",
        "\n",
        "\n",
        "class CAL(nn.Module):\n",
        "    def __init__(\n",
        "        self, backbone, backbone_out_feat,\n",
        "            num_classes, M=32, use_pytorch_resnet=True\n",
        "    ):\n",
        "        super(CAL, self).__init__()\n",
        "        self.use_pytorch_resnet = use_pytorch_resnet\n",
        "        self.num_classes = num_classes\n",
        "        self.M = M  # channels of attention maps\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "        # Network Initialization\n",
        "        self.features = backbone\n",
        "        self.num_features = backbone_out_feat\n",
        "\n",
        "        # Attention Maps\n",
        "        self.attentions = BasicConv2d(self.num_features,\n",
        "                                      self.M, kernel_size=1)\n",
        "\n",
        "        # Bilinear Attention Pooling\n",
        "        self.bap = BAP(pool=\"GAP\")\n",
        "\n",
        "        # Classification Layer\n",
        "        self.fc = nn.Linear(self.M * self.num_features,\n",
        "                            self.num_classes, bias=False)\n",
        "\n",
        "        print(\n",
        "            \"Using {} as feature extractor, num_classes: {}, \"\n",
        "            \"num_attentions: {}\".format(\n",
        "                type(self.features).__name__, self.num_classes, self.M\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def visualize(self, x):\n",
        "\n",
        "        # Feature Maps, Attention Maps and Feature Matrix\n",
        "        feature_maps = self.features(x)\n",
        "        if self.use_pytorch_resnet is True:\n",
        "            h_w = int(np.sqrt(feature_maps.size(1) // self.num_features))\n",
        "            feature_maps = feature_maps.view(\n",
        "                feature_maps.size(0), self.num_features, h_w, h_w\n",
        "            )\n",
        "        else:\n",
        "            feature_maps = feature_maps.view(\n",
        "                feature_maps.size(0), feature_maps.size(1), 1, 1\n",
        "            )\n",
        "        attention_maps = self.attentions(feature_maps)\n",
        "\n",
        "        feature_matrix = self.bap(feature_maps, attention_maps)\n",
        "        p = self.fc(feature_matrix * 100.0)\n",
        "\n",
        "        return p, attention_maps\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Feature Maps, Attention Maps and Feature Matrix\n",
        "        feature_maps = self.features(x)\n",
        "        if self.use_pytorch_resnet is True:\n",
        "            h_w = int(np.sqrt(feature_maps.size(1) // self.num_features))\n",
        "            feature_maps = feature_maps.view(\n",
        "                feature_maps.size(0), self.num_features, h_w, h_w\n",
        "            )\n",
        "        else:\n",
        "            feature_maps = feature_maps.view(\n",
        "                feature_maps.size(0), feature_maps.size(1), 1, 1\n",
        "            )\n",
        "\n",
        "        attention_maps = self.attentions(feature_maps)\n",
        "\n",
        "        feature_matrix, feature_matrix_hat = self.bap(\n",
        "            feature_maps, attention_maps)\n",
        "\n",
        "        # Classification\n",
        "        p = self.fc(feature_matrix * 100.0)\n",
        "\n",
        "        # Generate Attention Map\n",
        "        if self.training:\n",
        "            # Randomly choose one of attention maps Ak\n",
        "            attention_map = []\n",
        "            for i in range(batch_size):\n",
        "                tmp_map = attention_maps[i] - min(attention_maps[i].min(), 0)\n",
        "                attention_weights = torch.sqrt(\n",
        "                    tmp_map.sum(dim=(1, 2)).detach() + self.epsilon\n",
        "                )\n",
        "                attention_weights = F.normalize(attention_weights, p=1, dim=0)\n",
        "                k_index = np.random.choice(self.M, 2,\n",
        "                                           p=attention_weights.cpu().numpy())\n",
        "                attention_map.append(attention_maps[i, k_index, ...])\n",
        "            attention_map = torch.stack(\n",
        "                attention_map\n",
        "            )  # (B, 2, H, W) - one for cropping, the other for dropping\n",
        "        else:\n",
        "            attention_map = torch.mean(\n",
        "                attention_maps, dim=1, keepdim=True\n",
        "            )  # (B, 1, H, W)\n",
        "\n",
        "        return (p, p - self.fc(feature_matrix_hat * 100.0),\n",
        "                feature_matrix, attention_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOqx2dWxkQNs"
      },
      "source": [
        "### Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci7IHEh9qQON"
      },
      "source": [
        "def batch_augment(images, attention_map,\n",
        "                  mode=\"crop\", theta=0.5, padding_ratio=0.1):\n",
        "    batches, _, imgH, imgW = images.size()\n",
        "\n",
        "    if mode == \"crop\":\n",
        "        crop_images = []\n",
        "        for batch_index in range(batches):\n",
        "            atten_map = attention_map[batch_index:batch_index + 1]\n",
        "            if isinstance(theta, tuple):\n",
        "                theta_c = random.uniform(*theta) * atten_map.max()\n",
        "            else:\n",
        "                theta_c = theta * atten_map.max()\n",
        "\n",
        "            crop_mask = F.upsample_bilinear(atten_map,\n",
        "                                            size=(imgH, imgW)) >= theta_c\n",
        "            nonzero_indices = torch.nonzero(crop_mask[0, 0, ...])\n",
        "\n",
        "            # Special case: find no nonzero !!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "            if nonzero_indices[:, 0].numel() == 0:\n",
        "                crop_mask_h_siz = crop_mask.size(2)\n",
        "                crop_mask_w_siz = crop_mask.size(3)\n",
        "                assert crop_mask_h_siz == crop_mask_w_siz\n",
        "                crop_mask_siz_minus_one = crop_mask_h_siz - 1\n",
        "                nonzero_indices = torch.tensor(\n",
        "                    [\n",
        "                        [0, 0],\n",
        "                        [0, crop_mask_siz_minus_one],\n",
        "                        [crop_mask_siz_minus_one, 0],\n",
        "                        [crop_mask_siz_minus_one, crop_mask_siz_minus_one],\n",
        "                    ]\n",
        "                )\n",
        "\n",
        "            height_min = max(\n",
        "                int(nonzero_indices[:, 0].min().item() -\n",
        "                    padding_ratio * imgH), 0\n",
        "            )\n",
        "            height_max = min(\n",
        "                int(nonzero_indices[:, 0].max().item() +\n",
        "                    padding_ratio * imgH), imgH\n",
        "            )\n",
        "            width_min = max(\n",
        "                int(nonzero_indices[:, 1].min().item() -\n",
        "                    padding_ratio * imgW), 0\n",
        "            )\n",
        "            width_max = min(\n",
        "                int(nonzero_indices[:, 1].max().item() +\n",
        "                    padding_ratio * imgW), imgW\n",
        "            )\n",
        "\n",
        "            crop_images.append(\n",
        "                F.upsample_bilinear(\n",
        "                    images[\n",
        "                        batch_index:batch_index + 1,\n",
        "                        :,\n",
        "                        height_min:height_max,\n",
        "                        width_min:width_max,\n",
        "                    ],\n",
        "                    size=(imgH, imgW),\n",
        "                )\n",
        "            )\n",
        "        crop_images = torch.cat(crop_images, dim=0)\n",
        "        return crop_images\n",
        "\n",
        "    elif mode == \"drop\":\n",
        "        drop_masks = []\n",
        "        for batch_index in range(batches):\n",
        "            atten_map = attention_map[batch_index:batch_index + 1]\n",
        "            if isinstance(theta, tuple):\n",
        "                theta_d = random.uniform(*theta) * atten_map.max()\n",
        "            else:\n",
        "                theta_d = theta * atten_map.max()\n",
        "\n",
        "            drop_masks.append(\n",
        "                F.upsample_bilinear(atten_map, size=(imgH, imgW)) < theta_d\n",
        "            )\n",
        "        drop_masks = torch.cat(drop_masks, dim=0)\n",
        "        drop_images = images * drop_masks.float()\n",
        "        return drop_images\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Expected mode in ['crop', 'drop'], \"\n",
        "            \"but received unsupported augmentation method %s\"\n",
        "            % mode\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeo4OnsJw7oV"
      },
      "source": [
        "def eval_model(model, test_loader, device):\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            y_pred_raw, y_pred_aux, _, attention_map = model(images)\n",
        "\n",
        "            # crop_images3 = batch_augment(\n",
        "            #     images, attention_map, mode=\"crop\", theta=0.1,\n",
        "            #     padding_ratio=0.05\n",
        "            # )\n",
        "            # y_pred_crop3, y_pred_aux_crop3, _, _ = model(crop_images3)\n",
        "            #\n",
        "            # y_pred = (y_pred_raw + y_pred_crop3) / 2.0\n",
        "            # # y_pred_aux = (y_pred_aux + y_pred_aux_crop3) / 2.\n",
        "\n",
        "            _, predicted = torch.max(y_pred_raw.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_acc = correct / total\n",
        "    print(f\"Accuracy of the network on the test images: {test_acc:.4f}\")\n",
        "    return test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiaDTPUGcbjP"
      },
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.l2_loss = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        return self.l2_loss(outputs, targets) / outputs.size(0)\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    batch_size,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    n_epochs,\n",
        "    device,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    model_save_dir,\n",
        "    config_beta,\n",
        "    previous_record: dict = None,\n",
        "    no_valid=False,\n",
        "):\n",
        "    print(f'Batch size: {batch_size}, criterion: {criterion}')\n",
        "    if previous_record is not None:\n",
        "        losses = previous_record[\"losses\"]\n",
        "        accuracies = previous_record[\"accuracies\"]\n",
        "        test_accuracies = previous_record[\"test_accuracies\"]\n",
        "        feature_center = previous_record[\"feature_center\"]\n",
        "    else:\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        test_accuracies = []\n",
        "        feature_center = torch.zeros(\n",
        "            model.num_classes, model.M * model.num_features\n",
        "        ).to(device)\n",
        "\n",
        "    cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "    center_loss = CenterLoss()\n",
        "    # set the model to train mode initially\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\n",
        "            \"=== Epoch {} (lr={}) === \".format(\n",
        "                epoch + 1, optimizer.param_groups[0][\"lr\"]\n",
        "            ),\n",
        "            end=\"\",\n",
        "        )\n",
        "        since = time.time()\n",
        "        running_loss = 0.0\n",
        "        running_raw_loss = 0.0\n",
        "        running_aux_loss = 0.0\n",
        "        running_aug_loss = 0.0\n",
        "        running_feat_mat_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            if i % 5 == 0:\n",
        "                print(f\"{i} \", end=\"\")\n",
        "            # get the inputs and assign them to cuda\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #######################################################\n",
        "\n",
        "            y = labels\n",
        "            (y_pred_raw,\n",
        "             y_pred_aux,\n",
        "             feature_matrix, attention_map) = model(inputs)\n",
        "\n",
        "            # Update Feature Center\n",
        "            feature_center_batch = F.normalize(feature_center[y], dim=-1)\n",
        "            feature_center[y] += config_beta * (\n",
        "                feature_matrix.detach() - feature_center_batch\n",
        "            )\n",
        "\n",
        "            # Attention Cropping\n",
        "            with torch.no_grad():\n",
        "                crop_images = batch_augment(\n",
        "                    inputs,\n",
        "                    attention_map[:, :1, :, :],\n",
        "                    mode=\"crop\",\n",
        "                    theta=(0.4, 0.6),\n",
        "                    padding_ratio=0.1,\n",
        "                )\n",
        "                drop_images = batch_augment(\n",
        "                    inputs, attention_map[:, 1:, :, :], mode=\"drop\",\n",
        "                    theta=(0.2, 0.5)\n",
        "                )\n",
        "            aug_images = torch.cat([crop_images, drop_images], dim=0)\n",
        "            y_aug = torch.cat([y, y], dim=0)\n",
        "\n",
        "            # crop images forward\n",
        "            y_pred_aug, y_pred_aux_aug, _, _ = model(aug_images)\n",
        "\n",
        "            y_pred_aux = torch.cat([y_pred_aux, y_pred_aux_aug], dim=0)\n",
        "            y_aux = torch.cat([y, y_aug], dim=0)\n",
        "\n",
        "            # loss\n",
        "            raw_loss = cross_entropy_loss(y_pred_raw, y)\n",
        "            aux_loss = cross_entropy_loss(y_pred_aux, y_aux)\n",
        "            aug_loss = cross_entropy_loss(y_pred_aug, y_aug)\n",
        "            feat_mat_loss = center_loss(feature_matrix, feature_center_batch)\n",
        "            loss = (\n",
        "                raw_loss / 3.0\n",
        "                + aux_loss * 3.0 / 3.0\n",
        "                + aug_loss * 2.0 / 3.0\n",
        "                + feat_mat_loss\n",
        "            )\n",
        "            # loss = raw_loss / 3. + aux_loss  * 5. / 3. + \\\n",
        "            # aug_loss * 4. / 3. + feat_mat_loss\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, predicted = torch.max(y_pred_raw.data, 1)\n",
        "\n",
        "            # # forward + backward + optimize\n",
        "            # outputs = model(inputs)\n",
        "            # _, predicted = torch.max(outputs.data, 1)\n",
        "            # loss = criterion(outputs, labels)\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            # calculate the loss/acc later\n",
        "            running_loss += loss.item()\n",
        "            running_raw_loss += raw_loss.item()\n",
        "            running_aux_loss += aux_loss.item()\n",
        "            running_aug_loss += aug_loss.item()\n",
        "            running_feat_mat_loss += feat_mat_loss.item()\n",
        "            running_correct += (labels == predicted).sum().item()\n",
        "        print()\n",
        "\n",
        "        epoch_duration = time.time() - since\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_raw_loss = running_raw_loss / len(train_loader)\n",
        "        epoch_aux_loss = running_aux_loss / len(train_loader)\n",
        "        epoch_aug_loss = running_aug_loss / len(train_loader)\n",
        "        epoch_feat_mat_loss = running_feat_mat_loss / len(train_loader)\n",
        "        epoch_acc = running_correct / len(train_loader.dataset)\n",
        "        print(\n",
        "            \"(%d s) (loss: %.3f/%.3f/%.3f/%.3f) (acc: %.4f)\"\n",
        "            % (\n",
        "                epoch_duration,\n",
        "                epoch_raw_loss,\n",
        "                epoch_aux_loss,\n",
        "                epoch_aug_loss,\n",
        "                epoch_feat_mat_loss,\n",
        "                epoch_acc,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_acc)\n",
        "\n",
        "        if no_valid is not True:\n",
        "            # switch the model to eval mode to evaluate on test data\n",
        "            model.eval()\n",
        "            test_acc = eval_model(model, test_loader, device)\n",
        "            test_accuracies.append(test_acc)\n",
        "\n",
        "        # re-set the model to train mode after validating\n",
        "        model.train()\n",
        "        scheduler.step(test_acc)\n",
        "\n",
        "        model_save_name = \"{}_{:.4f}.pt\".format(\n",
        "            datetime.now().strftime(\"UTC+8_%Y_%m-%d_%H:%M\"), test_acc\n",
        "        )\n",
        "        model_save_path = os.path.join(model_save_dir, model_save_name)\n",
        "        # torch.save(model, model_save_path)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model,\n",
        "                \"losses\": losses,\n",
        "                \"accuracies\": accuracies,\n",
        "                \"test_accuracies\": test_accuracies,\n",
        "                \"feature_center\": feature_center,\n",
        "            },\n",
        "            model_save_path,\n",
        "        )\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "    return model, losses, accuracies, test_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSVkXC0_5xxE"
      },
      "source": [
        "### Only resnet train func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Vk3mvU5zhy"
      },
      "source": [
        "def eval_model_not_cal(model, test_loader, device):\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            y_pred = model(images)\n",
        "            # y_pred_aux = (y_pred_aux + y_pred_aux_crop3) / 2.\n",
        "\n",
        "            _, predicted = torch.max(y_pred.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    test_acc = correct / total\n",
        "    print(f\"Accuracy of the network on the test images: {test_acc:.4f}\")\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "def train_model_not_cal(\n",
        "    model,\n",
        "    batch_size,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    n_epochs,\n",
        "    device,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    model_save_dir,\n",
        "    config_beta,\n",
        "    previous_record: dict = None,\n",
        "    no_valid=False,\n",
        "):\n",
        "    print(f'Batch size: {batch_size}, config_beta: {config_beta}')\n",
        "    if previous_record is not None:\n",
        "\n",
        "        losses = previous_record[\"losses\"]\n",
        "        accuracies = previous_record[\"accuracies\"]\n",
        "        test_accuracies = previous_record[\"test_accuracies\"]\n",
        "    else:\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        test_accuracies = []\n",
        "\n",
        "    # set the model to train mode initially\n",
        "    model.train()\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\n",
        "            \"=== Epoch {} (lr={}) === \".format(\n",
        "                epoch + 1, optimizer.param_groups[0][\"lr\"]\n",
        "            ),\n",
        "            end=\"\",\n",
        "        )\n",
        "        since = time.time()\n",
        "        running_loss = 0.0\n",
        "        # running_raw_loss = 0.0\n",
        "        # running_aux_loss = 0.0\n",
        "        # running_aug_loss = 0.0\n",
        "        # running_feat_mat_loss = 0.0\n",
        "        running_correct = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            if i % 5 == 0:\n",
        "                print(f\"{i} \", end=\"\")\n",
        "            # get the inputs and assign them to cuda\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #######################################################\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # calculate the loss/acc later\n",
        "            running_loss += loss.item()\n",
        "            running_correct += (labels == predicted).sum().item()\n",
        "        print()\n",
        "\n",
        "        epoch_duration = time.time() - since\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_acc = running_correct / len(train_loader.dataset)\n",
        "        print(\n",
        "            \"(%d s) (loss: %.3f) (acc: %.4f)\" % (epoch_duration,\n",
        "                                                 epoch_loss, epoch_acc)\n",
        "        )\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_acc)\n",
        "\n",
        "        if no_valid is not True:\n",
        "            # switch the model to eval mode to evaluate on test data\n",
        "            model.eval()\n",
        "            test_acc = eval_model_not_cal(model, test_loader, device)\n",
        "            test_accuracies.append(test_acc)\n",
        "\n",
        "        # re-set the model to train mode after validating\n",
        "        model.train()\n",
        "        scheduler.step(test_acc)\n",
        "        since = time.time()\n",
        "\n",
        "        model_save_name = \"{}_{:.4f}.pt\".format(\n",
        "            datetime.now().strftime(\"UTC+8_%Y_%m-%d_%H:%M\"), test_acc\n",
        "        )\n",
        "        model_save_path = os.path.join(model_save_dir, model_save_name)\n",
        "        # torch.save(model, model_save_path)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\": model,\n",
        "                \"losses\": losses,\n",
        "                \"accuracies\": accuracies,\n",
        "                \"test_accuracies\": test_accuracies,\n",
        "            },\n",
        "            model_save_path,\n",
        "        )\n",
        "\n",
        "    print(\"Finished Training\")\n",
        "    return model, losses, accuracies, test_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2fzEiP6hFAU"
      },
      "source": [
        "# Training setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le8go0Ho0TH3"
      },
      "source": [
        "### Train from zero with CAL & WS-DAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13VQUioxgDao",
        "outputId": "2d5c5dd4-0916-4e81-9c9d-7662a2b1ac0b"
      },
      "source": [
        "# # Change backbone (effecient-net)\n",
        "# BACKBONE = EfficientNet.from_pretrained('efficientnet-b6')\n",
        "# BACKBONE_OUT_FEAT = BACKBONE._fc.in_features\n",
        "# BACKBONE._fc = Identity() # Remove\n",
        "# BACKBONE._swish = Identity() # Remove backbone's classifier\n",
        "# MODEL = CAL(backbone=BACKBONE, backbone_out_feat=BACKBONE_OUT_FEAT,\n",
        "#             num_classes=N_CLASSES, M=16, use_pytorch_resnet=False)\n",
        "# MODEL = MODEL.to(DEVICE)\n",
        "\n",
        "# Change backbone (ResNet)\n",
        "BACKBONE = models.resnet152(pretrained=True)\n",
        "BACKBONE_OUT_FEAT = BACKBONE.fc.in_features\n",
        "BACKBONE.avgpool = Identity()  # Remove\n",
        "BACKBONE.fc = Identity()  # Remove backbone's classifier\n",
        "MODEL = CAL(\n",
        "    backbone=BACKBONE,\n",
        "    backbone_out_feat=BACKBONE_OUT_FEAT,\n",
        "    num_classes=N_CLASSES,\n",
        "    M=16,\n",
        "    use_pytorch_resnet=True,\n",
        ")\n",
        "\n",
        "MODEL = MODEL.to(DEVICE)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "OPTIMIZER = optim.SGD(MODEL.parameters(), lr=LR, momentum=0.9)\n",
        "SCHEDULER = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    OPTIMIZER, mode=\"max\", patience=5, threshold=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using ResNet as feature extractor, num_classes: 200, num_attentions: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDVHPHJA5ZQJ"
      },
      "source": [
        "### Train from zero by only ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACwyFdWV5TDH"
      },
      "source": [
        "MODEL = models.resnet152(pretrained=True)\n",
        "MODEL = MODEL.to(DEVICE)\n",
        "BACKBONE_OUT_FEAT = MODEL.fc.in_features\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "OPTIMIZER = optim.SGD(MODEL.parameters(), lr=LR, momentum=0.9)\n",
        "SCHEDULER = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    OPTIMIZER, mode=\"max\", patience=4, threshold=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkFjqr1O0Yty"
      },
      "source": [
        "### Train a stored model (CAREFUL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcrIbXC8yYou"
      },
      "source": [
        "PREVIOUS_RECORD = torch.load(\n",
        "    \"/content/drive/MyDrive/2021VRDL/HW1/models/\"\n",
        "    \"UTC+8_2021_11-01_19:22_0.8950.pt\"\n",
        ")\n",
        "MODEL = PREVIOUS_RECORD[\"model\"]\n",
        "MODEL = MODEL.to(DEVICE)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "OPTIMIZER = optim.SGD(MODEL.parameters(), lr=LR, momentum=0.9)\n",
        "SCHEDULER = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    OPTIMIZER, mode=\"max\", patience=4, threshold=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkqYBntRhKG9"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waj_s_lfjlKL"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRvCMXnlOlcQ"
      },
      "source": [
        "MODEL, train_losses, train_accs, valid_accs = train_model(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    model=MODEL,\n",
        "    previous_record=None,\n",
        "    # previous_record=PREVIOUS_RECORD,\n",
        "    criterion=CRITERION,\n",
        "    optimizer=OPTIMIZER,\n",
        "    scheduler=SCHEDULER,\n",
        "    n_epochs=100,\n",
        "    device=DEVICE,\n",
        "    train_loader=TRAIN_DATALOADER,\n",
        "    test_loader=VALID_DATALOADER,\n",
        "    model_save_dir=MODEL_SAVE_DIR,\n",
        "    config_beta=5e-2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbnnnwNf0owO"
      },
      "source": [
        "### Change LR and nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfuauzr3_Pgo"
      },
      "source": [
        "def change_lr(optimizer, lr):\n",
        "    for g in optimizer.param_groups:\n",
        "        g[\"lr\"] = lr\n",
        "\n",
        "\n",
        "change_lr(OPTIMIZER, lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNcvCFd7zPmf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj26UAPlhPcm"
      },
      "source": [
        "# Predict & other tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h92P5nk0s4U"
      },
      "source": [
        "### Pred"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-Ocqzh7MQYG"
      },
      "source": [
        "def pred_images(model, test_loader, device):\n",
        "    all_predicted = []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            print(f\"Iter: {i}/{len(test_loader)}\")\n",
        "            images = data\n",
        "            images = images.to(device)\n",
        "\n",
        "            # y_pred_raw, y_pred_aux, _, attention_map = model(images)\n",
        "            # crop_images3 = batch_augment(images,\n",
        "            #                              attention_map, mode='crop',\n",
        "            #                              theta=0.1, padding_ratio=0.05)\n",
        "            # y_pred_crop3, y_pred_aux_crop3, _, _ = model(crop_images3)\n",
        "            # y_pred = (y_pred_raw + y_pred_crop3) / 2.\n",
        "\n",
        "            y_pred_raw, y_pred_aux, _, attention_map = model(images)\n",
        "            crop_images = batch_augment(\n",
        "                images, attention_map, mode=\"crop\",\n",
        "                theta=0.3, padding_ratio=0.1\n",
        "            )\n",
        "            y_pred_crop, y_pred_aux_crop, _, _ = model(crop_images)\n",
        "            crop_images2 = batch_augment(\n",
        "                images, attention_map, mode=\"crop\",\n",
        "                theta=0.2, padding_ratio=0.1\n",
        "            )\n",
        "            y_pred_crop2, y_pred_aux_crop2, _, _ = model(crop_images2)\n",
        "            crop_images3 = batch_augment(\n",
        "                images, attention_map, mode=\"crop\",\n",
        "                theta=0.1, padding_ratio=0.05\n",
        "            )\n",
        "            y_pred_crop3, y_pred_aux_crop3, _, _ = model(crop_images3)\n",
        "            y_pred = (y_pred_raw + y_pred_crop + \\\n",
        "                      y_pred_crop2 + y_pred_crop3) / 4.0\n",
        "\n",
        "            _, predicted = torch.max(y_pred.data, 1)\n",
        "            all_predicted += predicted.tolist()\n",
        "    return all_predicted\n",
        "\n",
        "\n",
        "def pred_images_not_cal(model, test_loader, device):\n",
        "    all_predicted = []\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(test_loader, 0):\n",
        "            print(f\"Iter: {i}/{len(test_loader)}\")\n",
        "            images = data\n",
        "            images = images.to(device)\n",
        "\n",
        "            y_pred = model(images)\n",
        "\n",
        "            _, predicted = torch.max(y_pred.data, 1)\n",
        "            all_predicted += predicted.tolist()\n",
        "    return all_predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlu47pJ1MYkG"
      },
      "source": [
        "predict = pred_images_not_cal(MODEL, TEST_DATALOADER, DEVICE)\n",
        "with open(\n",
        "    os.path.join(\n",
        "        PREDICTION_DIR, \"{}.txt\".format(\n",
        "            datetime.now().strftime(\"UTC+8_%Y_%m-%d_%H:%M\"))\n",
        "    ),\n",
        "    \"w+\",\n",
        ") as f:\n",
        "    content = \"\\n\".join(\n",
        "        [f\"{TEST_NAMES[i]} {CLS_LABEL_TO_NAME[item]}\"\n",
        "         for i, item in enumerate(predict)]\n",
        "    )\n",
        "    f.write(content)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfkiv-n1OnLK"
      },
      "source": [
        "predict = pred_images(MODEL, TEST_DATALOADER, DEVICE)\n",
        "with open(\n",
        "    os.path.join(\n",
        "        PREDICTION_DIR, \"{}.txt\".format(\n",
        "            datetime.now().strftime(\"UTC+8_%Y_%m-%d_%H:%M\"))\n",
        "    ),\n",
        "    \"w+\",\n",
        ") as f:\n",
        "    content = \"\\n\".join(\n",
        "        [f\"{TEST_NAMES[i]} {CLS_LABEL_TO_NAME[item]}\"\n",
        "         for i, item in enumerate(predict)]\n",
        "    )\n",
        "    f.write(content)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csXOe2wV0ue3"
      },
      "source": [
        "### Plot attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KCgp87NRThE"
      },
      "source": [
        "t = None\n",
        "for i, (inputs) in enumerate(TEST_DATALOADER):\n",
        "    if i < 3:\n",
        "        continue\n",
        "    _, c, h, w = inputs.shape\n",
        "    # t = inputs[0].view(1, c, h, w)\n",
        "    t = inputs\n",
        "    break\n",
        "t = t.to(DEVICE)\n",
        "\n",
        "\n",
        "def visualize(model, x):\n",
        "\n",
        "    # Feature Maps, Attention Maps and Feature Matrix\n",
        "    feature_maps = model.features(x)\n",
        "    if model.use_pytorch_resnet is True:\n",
        "        h_w = int(np.sqrt(feature_maps.size(1) // model.num_features))\n",
        "        feature_maps = feature_maps.view(\n",
        "            feature_maps.size(0), model.num_features, h_w, h_w\n",
        "        )\n",
        "    attention_maps = model.attentions(feature_maps)\n",
        "\n",
        "    feature_matrix = model.bap(feature_maps, attention_maps)\n",
        "    p = model.fc(feature_matrix[0] * 100.0)\n",
        "\n",
        "    return p, attention_maps\n",
        "\n",
        "\n",
        "ret_p, ret_atten_maps = visualize(x=t, model=MODEL)\n",
        "\n",
        "\n",
        "def generate_heatmap(attention_maps):\n",
        "    heat_attention_maps = []\n",
        "    heat_attention_maps.append(attention_maps[:, 0, ...])  # R\n",
        "    heat_attention_maps.append(\n",
        "        attention_maps[:, 0, ...] * (attention_maps[:, 0, ...] < 0.5).float()\n",
        "        + (1.0 - attention_maps[:, 0, ...]) * (\n",
        "                attention_maps[:, 0, ...] >= 0.5).float()\n",
        "    )  # G\n",
        "    heat_attention_maps.append(1.0 - attention_maps[:, 0, ...])  # B\n",
        "    return torch.stack(heat_attention_maps, dim=1)\n",
        "\n",
        "\n",
        "ToPILImage = transforms.ToPILImage()\n",
        "MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "STD = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    attention_maps = torch.max(ret_atten_maps, dim=1, keepdim=True)[0]\n",
        "    attention_maps = F.upsample_bilinear(attention_maps, size=(t.size(2),\n",
        "                                                               t.size(3)))\n",
        "    attention_maps = torch.sqrt(\n",
        "        attention_maps.cpu() / attention_maps.max().item())\n",
        "\n",
        "    heat_attention_maps = generate_heatmap(attention_maps)\n",
        "\n",
        "    # raw_image = t.cpu() * STD + MEAN\n",
        "    raw_image = t.cpu()\n",
        "    heat_attention_image = raw_image * 0.25 + heat_attention_maps * 0.75\n",
        "    raw_attention_image = raw_image * attention_maps\n",
        "\n",
        "    img_concat = None\n",
        "    for batch_idx in range(t.size(0)):\n",
        "        # rimg = ToPILImage(raw_image[batch_idx])\n",
        "        # haimg = ToPILImage(heat_attention_image[batch_idx])\n",
        "        savepath = \"/content/sample_data\"\n",
        "\n",
        "        if img_concat is None:\n",
        "            img_concat = raw_image[batch_idx].unsqueeze(dim=0)\n",
        "        else:\n",
        "            img_concat = torch.cat((img_concat,\n",
        "                                    raw_image[batch_idx].unsqueeze(dim=0)))\n",
        "        img_concat = torch.cat(\n",
        "            (img_concat, heat_attention_image[batch_idx].unsqueeze(dim=0))\n",
        "        )\n",
        "    grid = make_grid(img_concat)\n",
        "    save_image(grid, os.path.join(savepath, \"grid.png\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esm-bmotzoB-"
      },
      "source": [
        "### Show cropping examples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqtjgx7IzpzZ"
      },
      "source": [
        "t = None\n",
        "for i, (inputs) in enumerate(TEST_DATALOADER):\n",
        "    if i < 3:\n",
        "        continue\n",
        "    _, c, h, w = inputs.shape\n",
        "    t = inputs\n",
        "    break\n",
        "_, _, _, atm = MODEL(t.to(DEVICE))\n",
        "crop_images = batch_augment(t, atm, mode=\"drop\", theta=0.6, padding_ratio=0.1)\n",
        "\n",
        "\n",
        "grid = make_grid(torch.cat((t, crop_images)))\n",
        "arr = grid.cpu().numpy()\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(arr.transpose(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}