# -*- coding: utf-8 -*-
"""1103_hw1_inference.ipynb

Automatically generated by Colaboratory.
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import random
import time
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
from torch.utils.data import DataLoader, Dataset


class BirdDataset(Dataset):
    def __init__(self, img_dir, img_names, img_name_to_label,
                 transform, test=False):
        super().__init__()
        self.img_dir, self.img_names, self.img_name_to_label = (
            img_dir,
            img_names,
            img_name_to_label,
        )
        self.transform = transform
        self.test = test

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_arr = Image.open(
            os.path.join(self.img_dir, self.img_names[idx])).convert("RGB")
        if self.test is True:
            img_tensor = self.transform(img_arr)
            return img_tensor
        else:
            img_label = self.img_name_to_label[self.img_names[idx]]
            img_tensor = self.transform(img_arr)
            return img_tensor, img_label


#
class Identity(nn.Module):
    def __init__(self):
        super(Identity, self).__init__()

    def forward(self, x):
        return x


# Bilinear Attention Pooling
class BAP(nn.Module):
    def __init__(self, pool="GAP"):
        super(BAP, self).__init__()
        assert pool in ["GAP"]
        self.pool = None
        self.epsilon = 1e-6

    def forward(self, features, attentions):
        B, C, H, W = features.size()
        _, M, AH, AW = attentions.size()

        # match size
        if AH != H or AW != W:
            attentions = F.upsample_bilinear(attentions, size=(H, W))

        # feature_matrix: (B, M, C) -> (B, M * C)
        feature_matrix = (
                torch.einsum("imjk,injk->imn",
                             (attentions, features)) / float(H * W)
        ).view(B, -1)

        # sign-sqrt
        feature_matrix_raw = torch.sign(feature_matrix) * torch.sqrt(
            torch.abs(feature_matrix) + self.epsilon
        )

        # l2 normalization along dimension M and C
        feature_matrix = F.normalize(feature_matrix_raw, dim=-1)

        if self.training:
            fake_att = torch.zeros_like(attentions).uniform_(0, 2)
        else:
            fake_att = torch.ones_like(attentions)
        counterfactual_feature = (
                torch.einsum("imjk,injk->imn",
                             (fake_att, features)) / float(H * W)
        ).view(B, -1)

        counterfactual_feature = torch.sign(
            counterfactual_feature) * torch.sqrt(
            torch.abs(counterfactual_feature) + self.epsilon
        )

        counterfactual_feature = F.normalize(counterfactual_feature, dim=-1)
        return feature_matrix, counterfactual_feature


class BasicConv2d(nn.Module):
    def __init__(self, in_channels, out_channels, **kwargs):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)


class CAL(nn.Module):
    def __init__(
            self, backbone, backbone_out_feat,
            num_classes, M=32, use_pytorch_resnet=True
    ):
        super(CAL, self).__init__()
        self.use_pytorch_resnet = use_pytorch_resnet
        self.num_classes = num_classes
        self.M = M  # channels of attention maps
        self.epsilon = 1e-6

        # Network Initialization
        self.features = backbone
        self.num_features = backbone_out_feat

        # Attention Maps
        self.attentions = BasicConv2d(self.num_features,
                                      self.M, kernel_size=1)

        # Bilinear Attention Pooling
        self.bap = BAP(pool="GAP")

        # Classification Layer
        self.fc = nn.Linear(self.M * self.num_features,
                            self.num_classes, bias=False)

        print(
            "Using {} as feature extractor, num_classes: {}, "
            "num_attentions: {}".format(
                type(self.features).__name__, self.num_classes, self.M
            )
        )

    def visualize(self, x):

        # Feature Maps, Attention Maps and Feature Matrix
        feature_maps = self.features(x)
        if self.use_pytorch_resnet is True:
            h_w = int(np.sqrt(feature_maps.size(1) // self.num_features))
            feature_maps = feature_maps.view(
                feature_maps.size(0), self.num_features, h_w, h_w
            )
        else:
            feature_maps = feature_maps.view(
                feature_maps.size(0), feature_maps.size(1), 1, 1
            )
        attention_maps = self.attentions(feature_maps)

        feature_matrix = self.bap(feature_maps, attention_maps)
        p = self.fc(feature_matrix * 100.0)

        return p, attention_maps

    def forward(self, x):
        batch_size = x.size(0)

        # Feature Maps, Attention Maps and Feature Matrix
        feature_maps = self.features(x)
        if self.use_pytorch_resnet is True:
            h_w = int(np.sqrt(feature_maps.size(1) // self.num_features))
            feature_maps = feature_maps.view(
                feature_maps.size(0), self.num_features, h_w, h_w
            )
        else:
            feature_maps = feature_maps.view(
                feature_maps.size(0), feature_maps.size(1), 1, 1
            )

        attention_maps = self.attentions(feature_maps)

        feature_matrix, feature_matrix_hat = self.bap(
            feature_maps, attention_maps)

        # Classification
        p = self.fc(feature_matrix * 100.0)

        # Generate Attention Map
        if self.training:
            # Randomly choose one of attention maps Ak
            attention_map = []
            for i in range(batch_size):
                tmp_map = attention_maps[i] - min(attention_maps[i].min(), 0)
                attention_weights = torch.sqrt(
                    tmp_map.sum(dim=(1, 2)).detach() + self.epsilon
                )
                attention_weights = F.normalize(attention_weights, p=1, dim=0)
                k_index = np.random.choice(self.M, 2,
                                           p=attention_weights.cpu().numpy())
                attention_map.append(attention_maps[i, k_index, ...])
            attention_map = torch.stack(
                attention_map
            )  # (B, 2, H, W) - one for cropping, the other for dropping
        else:
            attention_map = torch.mean(
                attention_maps, dim=1, keepdim=True
            )  # (B, 1, H, W)

        return (p, p - self.fc(feature_matrix_hat * 100.0),
                feature_matrix, attention_map)


def batch_augment(images, attention_map,
                  mode="crop", theta=0.5, padding_ratio=0.1):
    batches, _, imgH, imgW = images.size()

    if mode == "crop":
        crop_images = []
        for batch_index in range(batches):
            atten_map = attention_map[batch_index:batch_index + 1]
            if isinstance(theta, tuple):
                theta_c = random.uniform(*theta) * atten_map.max()
            else:
                theta_c = theta * atten_map.max()

            crop_mask = F.upsample_bilinear(atten_map,
                                            size=(imgH, imgW)) >= theta_c
            nonzero_indices = torch.nonzero(crop_mask[0, 0, ...])

            # Special case: find no nonzero !!!!!!!!!!!!!!!!!!!!!!!!!
            if nonzero_indices[:, 0].numel() == 0:
                crop_mask_h_siz = crop_mask.size(2)
                crop_mask_w_siz = crop_mask.size(3)
                assert crop_mask_h_siz == crop_mask_w_siz
                crop_mask_siz_minus_one = crop_mask_h_siz - 1
                nonzero_indices = torch.tensor(
                    [
                        [0, 0],
                        [0, crop_mask_siz_minus_one],
                        [crop_mask_siz_minus_one, 0],
                        [crop_mask_siz_minus_one, crop_mask_siz_minus_one],
                    ]
                )

            height_min = max(
                int(nonzero_indices[:, 0].min().item() -
                    padding_ratio * imgH), 0
            )
            height_max = min(
                int(nonzero_indices[:, 0].max().item() +
                    padding_ratio * imgH), imgH
            )
            width_min = max(
                int(nonzero_indices[:, 1].min().item() -
                    padding_ratio * imgW), 0
            )
            width_max = min(
                int(nonzero_indices[:, 1].max().item() +
                    padding_ratio * imgW), imgW
            )

            crop_images.append(
                F.upsample_bilinear(
                    images[
                    batch_index:batch_index + 1,
                    :,
                    height_min:height_max,
                    width_min:width_max,
                    ],
                    size=(imgH, imgW),
                )
            )
        crop_images = torch.cat(crop_images, dim=0)
        return crop_images

    elif mode == "drop":
        drop_masks = []
        for batch_index in range(batches):
            atten_map = attention_map[batch_index:batch_index + 1]
            if isinstance(theta, tuple):
                theta_d = random.uniform(*theta) * atten_map.max()
            else:
                theta_d = theta * atten_map.max()

            drop_masks.append(
                F.upsample_bilinear(atten_map, size=(imgH, imgW)) < theta_d
            )
        drop_masks = torch.cat(drop_masks, dim=0)
        drop_images = images * drop_masks.float()
        return drop_images

    else:
        raise ValueError(
            "Expected mode in ['crop', 'drop'], "
            "but received unsupported augmentation method %s"
            #             % mode
        )


def inference(model_path: str,
              device,
              batch_size: int,
              testing_img_order_txt_file: str,
              testing_img_dir: str,
              classes_file: str,
              predict_save_dir: str = '.'):
    # Load labels and testing image names
    cls_label_to_name = {
        i: c
        for i, c in enumerate(pd.read_csv(
            classes_file, names=["class"])["class"].to_list())
    }
    CLS_NAME_TO_LABEL = {cls_label_to_name[i]: i for i in cls_label_to_name}
    test_names = pd.read_csv(testing_img_order_txt_file,
                             names=["filename"])["filename"].to_list()

    # Load model
    model = torch.load(model_path)['model'].to(device)

    # Prepare data
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((324, 324)),
        transforms.CenterCrop((300, 300)),
        transforms.Normalize((0.48194, 0.49756, 0.43214),
                             (0.13271, 0.12794, 0.17289)),
    ])
    test_dataset = BirdDataset(
        img_dir=testing_img_dir,
        img_names=test_names,
        img_name_to_label=None,
        transform=test_transform,
        test=True,
    )
    test_loader = DataLoader(test_dataset,
                             batch_size=batch_size,
                             shuffle=False)

    # Predict
    all_predicted = []
    with torch.no_grad():
        for i, data in enumerate(test_loader, 0):
            print(f"Iter: {i}/{len(test_loader)}")
            images = data
            images = images.to(device)

            y_pred_raw, y_pred_aux, _, attention_map = model(images)
            crop_images = batch_augment(
                images, attention_map, mode="crop",
                theta=0.3, padding_ratio=0.1
            )

            y_pred_crop, y_pred_aux_crop, _, _ = model(crop_images)
            crop_images2 = batch_augment(
                images, attention_map, mode="crop",
                theta=0.2, padding_ratio=0.1
            )

            y_pred_crop2, y_pred_aux_crop2, _, _ = model(crop_images2)
            crop_images3 = batch_augment(
                images, attention_map, mode="crop",
                theta=0.1, padding_ratio=0.05
            )
            y_pred_crop3, y_pred_aux_crop3, _, _ = model(crop_images3)

            y_pred = (y_pred_raw + y_pred_crop + \
                      y_pred_crop2 + y_pred_crop3) / 4.0

            _, predicted = torch.max(y_pred.data, 1)
            all_predicted += predicted.tolist()

    # Output
    with open(os.path.join("answer.txt"), "w+") as f:
        content = "\n".join([f"{test_names[i]} {cls_label_to_name[item]}"
                             for i, item in enumerate(all_predicted)])
        f.write(content)
    print("Done!")
    return


# Fill in the information first
MODEL_PATH = '...'
BATCH_SIZE = 8
TEST_IMG_ORDER_TXT_FILE = '.../testing_img_order.txt'
TEST_IMG_DIR = '.../testing'
CLASSES_TXT_FILE = '.../classes.txt'
PREDICT_SAVE_DIR = '.'

# Device
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print("Current device:", DEVICE)

# Start to predict
inference(model_path=MODEL_PATH,
          device=DEVICE,
          batch_size=BATCH_SIZE,
          testing_img_order_txt_file=TEST_IMG_ORDER_TXT_FILE,
          testing_img_dir=TEST_IMG_DIR,
          classes_file=CLASSES_TXT_FILE,
          predict_save_dir=PREDICT_SAVE_DIR)
